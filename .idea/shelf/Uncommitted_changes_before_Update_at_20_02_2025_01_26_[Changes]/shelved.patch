Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import tensorflow as tf\r\n\r\nfrom GPTauri import GPTauri\r\nfrom DataProcess import DataProcess\r\n\r\n\r\na = open(\"input_text.txt\", \"r\", encoding='utf-8').read()\r\n\r\nmodel = GPTauri()\r\ndata = DataProcess(a, context_length=model.CONFIGURATION[model.context_length])\r\n\r\ninput = None\r\n\r\nfor input_sample, output_sample in data.dataset:\r\n    input = input_sample\r\n\r\n\r\ndef generate_text(input_batches, num_of_additions):\r\n\r\n    for _ in range(num_of_additions):\r\n\r\n        context_len_input_batches = input_batches[:, -model.CONFIGURATION[model.context_length]:]\r\n\r\n        output = model(context_len_input_batches)\r\n        logits = output[:, -1, :]\r\n\r\n        probabilities = tf.keras.activations.softmax(logits, axis=-1)\r\n\r\n        next_word = tf.argmax(probabilities, axis=-1)\r\n        next_word = tf.expand_dims(next_word, axis=-1)\r\n\r\n        input_batches = tf.concat([input_batches, next_word], axis=1)\r\n\r\n    return input_batches\r\n\r\n\r\na = (generate_text(input, 10))\r\n\r\nfor idx in range(len(a)):\r\n    print(data.tokenizer.decode(a[idx][-100:]))\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision e2600da84c9747290540c52d21ca1d73ac3a88f5)
+++ b/main.py	(date 1740000294508)
@@ -3,25 +3,43 @@
 from GPTauri import GPTauri
 from DataProcess import DataProcess
 
+from ModelCrossEntropyLoss import CrossEntropyGPT
+from MultiHeadAttention import MultiHeadAttention
+from TransformerBlock import TransformerBlock
 
 a = open("input_text.txt", "r", encoding='utf-8').read()
 
 model = GPTauri()
-data = DataProcess(a, context_length=model.CONFIGURATION[model.context_length])
+
+data = DataProcess(
+    input_text=a,
+    stride=model.CONFIGURATION[model.context_length],
+    context_length=model.CONFIGURATION[model.context_length],
+    batch_size=8
+)
+
+
+def calculate_loss(input_batch, output_batch):
+    context_len_input_batches = input_batch[:, -model.CONFIGURATION[model.context_length]:]
+    context_len_output_batches = output_batch[:, -model.CONFIGURATION[model.context_length]:]
 
-input = None
+    context_len_output_batches = tf.reshape(context_len_output_batches, [-1])
+    one_hot_output = tf.one_hot(context_len_output_batches, model.CONFIGURATION[model.vocabulary_size])
 
-for input_sample, output_sample in data.dataset:
-    input = input_sample
+    logits = model(context_len_input_batches)
+    logits = tf.reshape(logits, [-1, model.CONFIGURATION[model.vocabulary_size]])
 
+    loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
 
-def generate_text(input_batches, num_of_additions):
+    return loss_func(logits, one_hot_output)
 
+
+def generate_text(input_batch, num_of_additions):
     for _ in range(num_of_additions):
-
-        context_len_input_batches = input_batches[:, -model.CONFIGURATION[model.context_length]:]
+        context_len_input_batches = input_batch[:, -model.CONFIGURATION[model.context_length]:]
 
         output = model(context_len_input_batches)
+
         logits = output[:, -1, :]
 
         probabilities = tf.keras.activations.softmax(logits, axis=-1)
@@ -29,12 +47,38 @@
         next_word = tf.argmax(probabilities, axis=-1)
         next_word = tf.expand_dims(next_word, axis=-1)
 
-        input_batches = tf.concat([input_batches, next_word], axis=1)
+        input_batch = tf.concat([input_batch, next_word], axis=1)
+
+    return input_batch
+
+
+def train_model():
+
+    inputs, outputs = 1, 1
+
+    for batch in data.dataset:
+        inputs, outputs = batch
+
+    model(inputs)
 
-    return input_batches
+    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
+                  loss=CrossEntropyGPT(model),
+                  metrics=['accuracy'],
+            )
 
+    model.summary()
 
-a = (generate_text(input, 10))
+    model.fit(data.dataset, epochs=1)
 
-for idx in range(len(a)):
-    print(data.tokenizer.decode(a[idx][-100:]))
+
+train_model()
+
+loss = 0
+
+print(len(data.dataset))
+for batch in data.dataset:
+    inputs, outputs = batch
+    loss += calculate_loss(inputs, outputs)
+    print(loss)
+
+print(loss)
Index: TransformerBlock.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import tensorflow as tf\r\n\r\nfrom MultiHeadAttention import MultiHeadAttention\r\n\r\n\r\nclass TransformerBlock(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, conf, **kwargs):\r\n\r\n        super().__init__(**kwargs)\r\n\r\n        self.norm_layer_1 = tf.keras.layers.LayerNormalization(\r\n            beta_initializer=\"random_uniform\",\r\n            gamma_initializer=\"random_uniform\"\r\n        )\r\n\r\n        self.multi_head_attention = MultiHeadAttention(\r\n            conf['embedding_dimension'],\r\n            conf['num_heads'],\r\n            conf['context_length'],\r\n            conf['drop_out_rate'],\r\n            conf['qkv_bias']\r\n        )\r\n\r\n        self.drop_out = tf.keras.layers.Dropout(conf[\"drop_out_rate\"])\r\n\r\n        self.norm_layer_2 = tf.keras.layers.LayerNormalization(\r\n            beta_initializer=\"random_uniform\",\r\n            gamma_initializer=\"random_uniform\"\r\n        )\r\n\r\n        self.linear_layer_1 = tf.keras.layers.Dense(conf['embedding_dimension'] * 4)\r\n        self.gelu_activation = tf.keras.activations.gelu\r\n        self.linear_layer_2 = tf.keras.layers.Dense(conf['embedding_dimension'])\r\n\r\n    def call(self, input_data):\r\n\r\n        shortcut = input_data\r\n\r\n        x = self.norm_layer_1(shortcut)\r\n        x = self.multi_head_attention(x)\r\n        x = self.drop_out(x)\r\n\r\n        x = x + shortcut\r\n        shortcut = x\r\n\r\n        x = self.norm_layer_2(x)\r\n        x = self.linear_layer_1(x)\r\n        x = self.gelu_activation(x)\r\n        x = self.linear_layer_2(x)\r\n        x = self.drop_out(x)\r\n\r\n        x = x + shortcut\r\n\r\n        return x\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/TransformerBlock.py b/TransformerBlock.py
--- a/TransformerBlock.py	(revision e2600da84c9747290540c52d21ca1d73ac3a88f5)
+++ b/TransformerBlock.py	(date 1739963159426)
@@ -7,7 +7,7 @@
 
     def __init__(self, conf, **kwargs):
 
-        super().__init__(**kwargs)
+        super(TransformerBlock, self).__init__(**kwargs)
 
         self.norm_layer_1 = tf.keras.layers.LayerNormalization(
             beta_initializer="random_uniform",
Index: GPTauri.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import tensorflow as tf\r\nfrom TransformerBlock import TransformerBlock\r\n\r\n\r\nclass GPTauri(tf.keras.layers.Layer):\r\n\r\n    CONFIGURATION = {\r\n        \"vocabulary_size\": 50257,\r\n        \"embedding_dimension\": 768,\r\n        \"num_heads\": 12,\r\n        \"context_length\": 1024,\r\n        \"drop_out_rate\": 0.1,\r\n        \"qkv_bias\": False,\r\n        \"num_layers\": 12\r\n    }\r\n\r\n    vocabulary_size = 'vocabulary_size'\r\n    embedding_dimension = 'embedding_dimension'\r\n    num_heads = 'num_heads'\r\n    context_length = 'context_length'\r\n    drop_out_rate = 'drop_out_rate'\r\n    qkv_bias = 'qkv_bias'\r\n    num_layers = 'num_layers'\r\n\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        self.token_embedding_layer = tf.keras.layers.Embedding(\r\n            input_dim=self.CONFIGURATION[self.vocabulary_size],\r\n            output_dim=self.CONFIGURATION[self.embedding_dimension]\r\n        )\r\n\r\n        self.position_embedding_layer = tf.keras.layers.Embedding(\r\n            input_dim=self.CONFIGURATION[self.context_length],\r\n            output_dim=self.CONFIGURATION[self.embedding_dimension]\r\n        )\r\n\r\n        self.drop_out_layer = tf.keras.layers.Dropout(self.CONFIGURATION[self.drop_out_rate])\r\n\r\n        self.transformer_blocks = [TransformerBlock(self.CONFIGURATION)\r\n                                   for _ in range(self.CONFIGURATION[self.num_layers])]\r\n\r\n        self.normalization_layer = tf.keras.layers.LayerNormalization(\r\n            beta_initializer=\"random_uniform\",\r\n            gamma_initializer=\"random_uniform\"\r\n        )\r\n\r\n        self.linear_output_layer = tf.keras.layers.Dense(self.CONFIGURATION[self.vocabulary_size])\r\n\r\n    def build(self, input_shape):\r\n        super().build(input_shape)\r\n\r\n    def call(self, input_data):\r\n\r\n        batch_size, sentence_len = input_data.shape\r\n\r\n        token_embeddings = self.token_embedding_layer(input_data)\r\n        positional_embeddings = self.position_embedding_layer(tf.range(0, sentence_len))\r\n        x = token_embeddings + positional_embeddings\r\n\r\n        x = self.drop_out_layer(x)\r\n\r\n        for idx in range(len(self.transformer_blocks)):\r\n            x = self.transformer_blocks[idx](x)\r\n\r\n        x = self.normalization_layer(x)\r\n        logits = self.linear_output_layer(x)\r\n\r\n        return logits\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/GPTauri.py b/GPTauri.py
--- a/GPTauri.py	(revision e2600da84c9747290540c52d21ca1d73ac3a88f5)
+++ b/GPTauri.py	(date 1739999082437)
@@ -2,13 +2,12 @@
 from TransformerBlock import TransformerBlock
 
 
-class GPTauri(tf.keras.layers.Layer):
-
+class GPTauri(tf.keras.Model):
     CONFIGURATION = {
         "vocabulary_size": 50257,
         "embedding_dimension": 768,
         "num_heads": 12,
-        "context_length": 1024,
+        "context_length": 128,  # 1024,
         "drop_out_rate": 0.1,
         "qkv_bias": False,
         "num_layers": 12
@@ -23,7 +22,7 @@
     num_layers = 'num_layers'
 
     def __init__(self):
-        super().__init__()
+        super(GPTauri, self).__init__()
 
         self.token_embedding_layer = tf.keras.layers.Embedding(
             input_dim=self.CONFIGURATION[self.vocabulary_size],
@@ -45,13 +44,11 @@
             gamma_initializer="random_uniform"
         )
 
-        self.linear_output_layer = tf.keras.layers.Dense(self.CONFIGURATION[self.vocabulary_size])
-
-    def build(self, input_shape):
-        super().build(input_shape)
+        self.linear_output_layer = tf.keras.layers.Dense(self.CONFIGURATION[self.vocabulary_size],
+                                                         kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,
+                                                                                                               stddev=0.05))
 
     def call(self, input_data):
-
         batch_size, sentence_len = input_data.shape
 
         token_embeddings = self.token_embedding_layer(input_data)
Index: MultiHeadAttention.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\n\r\nclass MultiHeadAttention(tf.keras.layers.Layer):\r\n\r\n    def __init__(self, qkv_dim, num_heads, context_len, dropout_rate, qkv_bias=False, **kwargs):\r\n\r\n        super().__init__(**kwargs)\r\n\r\n        if qkv_dim % num_heads != 0:\r\n            raise ArithmeticError(\"Dimensions do not align, each head needs equal amount of dimensions\")\r\n\r\n        self.qkv_dim = qkv_dim\r\n        self.num_heads = num_heads\r\n        self.head_dim = qkv_dim // num_heads\r\n\r\n        self.query_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias)\r\n        self.key_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias)\r\n        self.value_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias)\r\n\r\n        self.mask = tf.linalg.band_part(tf.ones((context_len, context_len)), -1, 0)\r\n        self.mask = tf.where(self.mask == 0, tf.constant(-float('inf')), tf.constant(0.0))\r\n        self.mask = tf.reshape(self.mask, [1, 1, context_len, context_len])\r\n\r\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\r\n\r\n        self.out_proj = tf.keras.layers.Dense(units=self.qkv_dim)\r\n\r\n    def call(self, input_data):\r\n\r\n        batch, context_len, embedding_dim = input_data.shape\r\n\r\n        query = self.query_weights(input_data)\r\n        key = self.key_weights(input_data)\r\n        value = self.value_weights(input_data)\r\n\r\n        query = tf.reshape(query, [batch, context_len, self.num_heads, self.head_dim])\r\n        key = tf.reshape(key, [batch, context_len, self.num_heads, self.head_dim])\r\n        value = tf.reshape(value, [batch, context_len, self.num_heads, self.head_dim])\r\n\r\n        query = tf.transpose(query, perm=[0, 2, 1, 3])\r\n        key = tf.transpose(key, perm=[0, 2, 1, 3])\r\n        value = tf.transpose(value, perm=[0, 2, 1, 3])\r\n\r\n        attention_scores = tf.matmul(query, tf.transpose(key, [0, 1, 3, 2]))\r\n        attention_scores += self.mask\r\n\r\n        attention_weights = tf.nn.softmax(attention_scores / self.head_dim ** 0.5, axis=-1)\r\n        attention_weights = self.dropout(attention_weights)\r\n\r\n        context_vecs = tf.matmul(attention_weights, value)\r\n        context_vecs = tf.transpose(context_vecs, perm=[0, 2, 1, 3])\r\n\r\n        context_vecs = tf.reshape(context_vecs, [batch, context_len, self.qkv_dim])\r\n        context_vecs = self.out_proj(context_vecs)\r\n\r\n        return context_vecs\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/MultiHeadAttention.py b/MultiHeadAttention.py
--- a/MultiHeadAttention.py	(revision e2600da84c9747290540c52d21ca1d73ac3a88f5)
+++ b/MultiHeadAttention.py	(date 1739963047214)
@@ -1,4 +1,3 @@
-
 import tensorflow as tf
 import numpy as np
 
@@ -6,8 +5,7 @@
 class MultiHeadAttention(tf.keras.layers.Layer):
 
     def __init__(self, qkv_dim, num_heads, context_len, dropout_rate, qkv_bias=False, **kwargs):
-
-        super().__init__(**kwargs)
+        super(MultiHeadAttention, self).__init__(**kwargs)
 
         if qkv_dim % num_heads != 0:
             raise ArithmeticError("Dimensions do not align, each head needs equal amount of dimensions")
@@ -16,9 +14,15 @@
         self.num_heads = num_heads
         self.head_dim = qkv_dim // num_heads
 
-        self.query_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias)
-        self.key_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias)
-        self.value_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias)
+        self.query_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias,
+                                                   kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,
+                                                                                                         stddev=0.05))
+        self.key_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias,
+                                                 kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,
+                                                                                                       stddev=0.05))
+        self.value_weights = tf.keras.layers.Dense(units=qkv_dim, use_bias=qkv_bias,
+                                                   kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,
+                                                                                                         stddev=0.05))
 
         self.mask = tf.linalg.band_part(tf.ones((context_len, context_len)), -1, 0)
         self.mask = tf.where(self.mask == 0, tf.constant(-float('inf')), tf.constant(0.0))
@@ -26,11 +30,14 @@
 
         self.dropout = tf.keras.layers.Dropout(dropout_rate)
 
-        self.out_proj = tf.keras.layers.Dense(units=self.qkv_dim)
+        self.out_proj = tf.keras.layers.Dense(units=self.qkv_dim,
+                                              kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0,
+                                                                                                    stddev=0.05))
 
     def call(self, input_data):
-
-        batch, context_len, embedding_dim = input_data.shape
+        batch = tf.shape(input_data)[0]  # Get dynamic batch size
+        context_len = tf.shape(input_data)[1]  # Get dynamic context length
+        embedding_dim = tf.shape(input_data)[2]
 
         query = self.query_weights(input_data)
         key = self.key_weights(input_data)
@@ -57,4 +64,3 @@
         context_vecs = self.out_proj(context_vecs)
 
         return context_vecs
-
Index: ModelCrossEntropyLoss.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/ModelCrossEntropyLoss.py b/ModelCrossEntropyLoss.py
new file mode 100644
--- /dev/null	(date 1739999239451)
+++ b/ModelCrossEntropyLoss.py	(date 1739999239451)
@@ -0,0 +1,21 @@
+import tensorflow as tf
+
+
+class CrossEntropyGPT(tf.keras.losses.Loss):
+    def __init__(self, model, name="CrossEntropyGPT"):
+        super(CrossEntropyGPT, self).__init__(name=name)
+        self.model = model
+
+    def call(self, output_batch, logits):
+        context_len_output_batches = output_batch[:, -self.model.CONFIGURATION[self.model.context_length]:]
+
+        context_len_output_batches = tf.reshape(context_len_output_batches, [-1])
+        context_len_output_batches = tf.cast(context_len_output_batches, tf.int32)
+
+        one_hot_output = tf.one_hot(context_len_output_batches, self.model.CONFIGURATION[self.model.vocabulary_size])
+
+        logits = tf.reshape(logits, [-1, self.model.CONFIGURATION[self.model.vocabulary_size]])
+
+        loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
+
+        return loss_func(logits, one_hot_output)
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"7dc88029-b3ff-49ac-a473-8b02de5fa6d3\" name=\"Changes\" comment=\"\">\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/GPTauri.iml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/inspectionProfiles/Project_Default.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/inspectionProfiles/profiles_settings.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/misc.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/modules.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/vcs.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/DataProcess.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/GPTauri.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/MultiHeadAttention.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/Normalization.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/TransformerBlock.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/main.py\" afterDir=\"false\" />\r\n      <change afterPath=\"$PROJECT_DIR$/utils.py\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"HighlightingSettingsPerFile\">\r\n    <setting file=\"file://$PROJECT_DIR$/.venv/Lib/site-packages/keras/src/layers/layer.py\" root0=\"SKIP_INSPECTION\" />\r\n  </component>\r\n  <component name=\"ProjectColorInfo\">{\r\n  &quot;associatedIndex&quot;: 8\r\n}</component>\r\n  <component name=\"ProjectId\" id=\"2rBDZvT6EgoV6knMGE5q5UbBOQD\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\">{\r\n  &quot;keyToString&quot;: {\r\n    &quot;Python.Preprocess.executor&quot;: &quot;Run&quot;,\r\n    &quot;Python.main.executor&quot;: &quot;Run&quot;,\r\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\r\n    &quot;git-widget-placeholder&quot;: &quot;master&quot;,\r\n    &quot;last_opened_file_path&quot;: &quot;D:/programebi/Python/Chud GPTauri&quot;\r\n  }\r\n}</component>\r\n  <component name=\"SharedIndexes\">\r\n    <attachedChunks>\r\n      <set>\r\n        <option value=\"bundled-python-sdk-0509580d9d50-746f403e7f0c-com.jetbrains.pycharm.community.sharedIndexes.bundled-PC-241.14494.241\" />\r\n      </set>\r\n    </attachedChunks>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"7dc88029-b3ff-49ac-a473-8b02de5fa6d3\" name=\"Changes\" comment=\"\" />\r\n      <created>1736023633710</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1736023633710</updated>\r\n    </task>\r\n    <servers />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision e2600da84c9747290540c52d21ca1d73ac3a88f5)
+++ b/.idea/workspace.xml	(date 1739989570700)
@@ -4,21 +4,13 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="7dc88029-b3ff-49ac-a473-8b02de5fa6d3" name="Changes" comment="">
-      <change afterPath="$PROJECT_DIR$/.idea/GPTauri.iml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/inspectionProfiles/Project_Default.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/inspectionProfiles/profiles_settings.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/modules.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/vcs.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/DataProcess.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/GPTauri.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/MultiHeadAttention.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/Normalization.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/TransformerBlock.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/main.py" afterDir="false" />
-      <change afterPath="$PROJECT_DIR$/utils.py" afterDir="false" />
+    <list default="true" id="7dc88029-b3ff-49ac-a473-8b02de5fa6d3" name="Changes" comment="commited files">
+      <change afterPath="$PROJECT_DIR$/ModelCrossEntropyLoss.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/GPTauri.py" beforeDir="false" afterPath="$PROJECT_DIR$/GPTauri.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/MultiHeadAttention.py" beforeDir="false" afterPath="$PROJECT_DIR$/MultiHeadAttention.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/TransformerBlock.py" beforeDir="false" afterPath="$PROJECT_DIR$/TransformerBlock.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/main.py" beforeDir="false" afterPath="$PROJECT_DIR$/main.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -50,12 +42,38 @@
   <component name="PropertiesComponent">{
   &quot;keyToString&quot;: {
     &quot;Python.Preprocess.executor&quot;: &quot;Run&quot;,
+    &quot;Python.TransformerBlock.executor&quot;: &quot;Run&quot;,
     &quot;Python.main.executor&quot;: &quot;Run&quot;,
     &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,
     &quot;git-widget-placeholder&quot;: &quot;master&quot;,
-    &quot;last_opened_file_path&quot;: &quot;D:/programebi/Python/Chud GPTauri&quot;
+    &quot;last_opened_file_path&quot;: &quot;D:/programebi/Python/Chud GPTauri/main.py&quot;
   }
 }</component>
+  <component name="RunManager">
+    <configuration name="main" type="PythonConfigurationType" factoryName="Python">
+      <module name="GPTauri" />
+      <option name="ENV_FILES" value="" />
+      <option name="INTERPRETER_OPTIONS" value="" />
+      <option name="PARENT_ENVS" value="true" />
+      <envs>
+        <env name="PYTHONUNBUFFERED" value="1" />
+      </envs>
+      <option name="SDK_HOME" value="" />
+      <option name="SDK_NAME" value="Python 3.12 (Chud GPTauri)" />
+      <option name="WORKING_DIRECTORY" value="" />
+      <option name="IS_MODULE_SDK" value="false" />
+      <option name="ADD_CONTENT_ROOTS" value="true" />
+      <option name="ADD_SOURCE_ROOTS" value="true" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/main.py" />
+      <option name="PARAMETERS" value="" />
+      <option name="SHOW_COMMAND_LINE" value="false" />
+      <option name="EMULATE_TERMINAL" value="false" />
+      <option name="MODULE_MODE" value="false" />
+      <option name="REDIRECT_INPUT" value="false" />
+      <option name="INPUT_FILE" value="" />
+      <method v="2" />
+    </configuration>
+  </component>
   <component name="SharedIndexes">
     <attachedChunks>
       <set>
@@ -72,6 +90,19 @@
       <option name="presentableId" value="Default" />
       <updated>1736023633710</updated>
     </task>
+    <task id="LOCAL-00001" summary="commited files">
+      <option name="closed" value="true" />
+      <created>1739488408829</created>
+      <option name="number" value="00001" />
+      <option name="presentableId" value="LOCAL-00001" />
+      <option name="project" value="LOCAL" />
+      <updated>1739488408830</updated>
+    </task>
+    <option name="localTasksCounter" value="2" />
     <servers />
   </component>
+  <component name="VcsManagerConfiguration">
+    <MESSAGE value="commited files" />
+    <option name="LAST_COMMIT_MESSAGE" value="commited files" />
+  </component>
 </project>
\ No newline at end of file
